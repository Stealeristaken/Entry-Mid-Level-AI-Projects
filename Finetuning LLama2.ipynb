{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Finetuning LLama2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stealeristaken/Entry-Mid-Level-AI-Projects/blob/main/Finetuning%20LLama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U transformers datasets accelerate peft trl bitsandbytes wandb"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "JW15Il2Kln8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = 'your_token_here'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:24:51.008539Z",
          "iopub.execute_input": "2024-03-06T16:24:51.009584Z",
          "iopub.status.idle": "2024-03-06T16:24:51.015194Z",
          "shell.execute_reply.started": "2024-03-06T16:24:51.009524Z",
          "shell.execute_reply": "2024-03-06T16:24:51.014102Z"
        },
        "trusted": true,
        "id": "I2oqe5BTln8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T13:03:00.001596Z",
          "iopub.execute_input": "2024-03-06T13:03:00.00251Z",
          "iopub.status.idle": "2024-03-06T13:03:09.495897Z",
          "shell.execute_reply.started": "2024-03-06T13:03:00.002472Z",
          "shell.execute_reply": "2024-03-06T13:03:09.494607Z"
        },
        "trusted": true,
        "id": "8v9pAxURln8i",
        "outputId": "d1969b8c-d68e-4664-f910-719abfb5a4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-06 13:03:04.974853: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-06 13:03:04.974909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-06 13:03:04.976293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = 'NousResearch/Llama-2-7b-hf'\n",
        "new_model = 'llama-2-7b-miniplatypus'\n",
        "\n",
        "dataset = load_dataset('ardaorcun/mini-platypus-tryout', split=\"train\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T13:03:09.498182Z",
          "iopub.execute_input": "2024-03-06T13:03:09.498606Z",
          "iopub.status.idle": "2024-03-06T13:03:11.844704Z",
          "shell.execute_reply.started": "2024-03-06T13:03:09.498547Z",
          "shell.execute_reply": "2024-03-06T13:03:11.843623Z"
        },
        "trusted": true,
        "id": "xsQj_69Iln8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T13:03:11.846018Z",
          "iopub.execute_input": "2024-03-06T13:03:11.84638Z",
          "iopub.status.idle": "2024-03-06T13:03:18.013697Z",
          "shell.execute_reply.started": "2024-03-06T13:03:11.846348Z",
          "shell.execute_reply": "2024-03-06T13:03:18.01289Z"
        },
        "trusted": true,
        "id": "xqWvTDqOln8i",
        "outputId": "dcf56199-45f7-43b6-af07-00fdb468047c",
        "colab": {
          "referenced_widgets": [
            "664ba3bfc45d444099ce5e184f2717e1"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664ba3bfc45d444099ce5e184f2717e1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments=TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 4,\n",
        "    per_device_train_batch_size = 10,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    evaluation_strategy = \"steps\",\n",
        "    eval_steps = 1000,\n",
        "    logging_steps = 1,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    learning_rate = 2e-4,\n",
        "    lr_scheduler_type = 'linear',\n",
        "    warmup_steps = 10,\n",
        "    report_to = 'wandb',\n",
        "    #max_steps = 3 #gonna remove for full training\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = dataset,\n",
        "    peft_config = peft_config,\n",
        "    dataset_text_field = \"instruction\",\n",
        "    max_seq_length = 512,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_arguments\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T13:03:18.015763Z",
          "iopub.execute_input": "2024-03-06T13:03:18.01641Z",
          "iopub.status.idle": "2024-03-06T16:21:50.699678Z",
          "shell.execute_reply.started": "2024-03-06T13:03:18.016371Z",
          "shell.execute_reply": "2024-03-06T16:21:50.698638Z"
        },
        "trusted": true,
        "id": "4f3yRLu3ln8i",
        "outputId": "0469c0b9-1c41-4ce2-8baf-d0d8cc317db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marda-orcun\u001b[0m (\u001b[33mdata-leak\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.4"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240306_130320-ruoaji7x</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/data-leak/huggingface/runs/ruoaji7x' target=\"_blank\">peachy-waterfall-3</a></strong> to <a href='https://wandb.ai/data-leak/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/data-leak/huggingface' target=\"_blank\">https://wandb.ai/data-leak/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/data-leak/huggingface/runs/ruoaji7x' target=\"_blank\">https://wandb.ai/data-leak/huggingface/runs/ruoaji7x</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [400/400 3:17:27, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is a large language model?\"\n",
        "instruction = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
        "result = pipe(instruction)\n",
        "print(result[0]['generated_text'][len(instruction):])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:22:06.381359Z",
          "iopub.execute_input": "2024-03-06T16:22:06.382294Z",
          "iopub.status.idle": "2024-03-06T16:22:56.843157Z",
          "shell.execute_reply.started": "2024-03-06T16:22:06.382255Z",
          "shell.execute_reply": "2024-03-06T16:22:56.842019Z"
        },
        "trusted": true,
        "id": "rFDEiFPQln8j",
        "outputId": "ebb05f1a-4fea-4986-ea6f-18c9618633e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nA large language model (LLM) is a type of natural language processing (NLP) model that uses deep learning techniques to generate human-like text. These models are trained on vast amounts of text data, typically several billion words or more, allowing them to learn complex patterns and relationships between words and phrases.\n\nLLMs have become increasingly popular in recent years due to their ability to produce high-quality text on a wide range of topics. They can be used for tasks such as text generation, question answering,\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:24:08.22148Z",
          "iopub.execute_input": "2024-03-06T16:24:08.222385Z",
          "iopub.status.idle": "2024-03-06T16:24:08.943014Z",
          "shell.execute_reply.started": "2024-03-06T16:24:08.22235Z",
          "shell.execute_reply": "2024-03-06T16:24:08.941852Z"
        },
        "trusted": true,
        "id": "MkuX8XX_ln8j",
        "outputId": "4792033c-6ec5-4f70-f514-de5bbd8db6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:24:18.431191Z",
          "iopub.execute_input": "2024-03-06T16:24:18.431593Z",
          "iopub.status.idle": "2024-03-06T16:24:25.520295Z",
          "shell.execute_reply.started": "2024-03-06T16:24:18.43153Z",
          "shell.execute_reply": "2024-03-06T16:24:25.519306Z"
        },
        "trusted": true,
        "id": "tZ-hfMgTln8j",
        "outputId": "f7e49db5-12c6-4d3a-d832-a10aaa10bfac",
        "colab": {
          "referenced_widgets": [
            "87aada1dce6b434c8f9304583159dfe7"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87aada1dce6b434c8f9304583159dfe7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#additional\n",
        "#model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n",
        "#tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-06T16:25:44.938613Z",
          "iopub.execute_input": "2024-03-06T16:25:44.939077Z",
          "iopub.status.idle": "2024-03-06T16:25:45.49235Z",
          "shell.execute_reply.started": "2024-03-06T16:25:44.939042Z",
          "shell.execute_reply": "2024-03-06T16:25:45.490474Z"
        },
        "trusted": true,
        "id": "5A25n4fEln8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbjZadRoln8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}